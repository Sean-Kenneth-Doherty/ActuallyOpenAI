"""
Test script for the ActuallyOpenAI self-improving training system.
"""

import asyncio
import sys
import torch

print("=" * 60)
print("ActuallyOpenAI - Self-Improving AI Training System Test")
print("=" * 60)

# Test 1: Import all modules
print("\n[1/6] Testing imports...")
try:
    from actuallyopenai.models import AOAIModel, ModelConfig, create_model
    from actuallyopenai.training import (
        ContinuousTrainer,
        FederatedAggregator,
        ModelEvolution,
        AutoScalingController,
        ImprovementTracker,
        AggregationStrategy,
        ScalingMode,
        TrainingPhase,
    )
    print("‚úÖ All imports successful!")
except ImportError as e:
    print(f"‚ùå Import error: {e}")
    sys.exit(1)

# Test 2: Create model
print("\n[2/6] Testing model creation...")
try:
    config = ModelConfig.tiny()  # Use tiny for testing
    model = AOAIModel(config)
    print(f"‚úÖ Model created: {model.num_parameters:,} parameters")
    print(f"   - Hidden size: {config.hidden_size}")
    print(f"   - Layers: {config.num_layers}")
    print(f"   - Heads: {config.num_heads}")
except Exception as e:
    print(f"‚ùå Model creation error: {e}")
    sys.exit(1)

# Test 3: Test forward pass
print("\n[3/6] Testing model forward pass...")
try:
    batch_size, seq_len = 2, 64
    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))
    labels = torch.randint(0, config.vocab_size, (batch_size, seq_len))
    
    logits, loss = model(input_ids, labels=labels)
    print(f"‚úÖ Forward pass successful!")
    print(f"   - Input shape: {input_ids.shape}")
    print(f"   - Output shape: {logits.shape}")
    print(f"   - Loss: {loss.item():.4f}")
except Exception as e:
    print(f"‚ùå Forward pass error: {e}")
    sys.exit(1)

# Test 4: Test training components
print("\n[4/6] Testing training components...")
try:
    # Continuous Trainer
    trainer = ContinuousTrainer(
        model=model,
        model_id="test-model",
        checkpoint_dir="./test_checkpoints",
        min_workers=1,
    )
    print(f"‚úÖ ContinuousTrainer initialized")
    print(f"   - Phase: {trainer.state.phase.value}")
    
    # Federated Aggregator
    aggregator = FederatedAggregator(
        strategy=AggregationStrategy.FEDAVG,
        min_workers=2,
    )
    print(f"‚úÖ FederatedAggregator initialized")
    print(f"   - Strategy: {aggregator.strategy.value}")
    
    # Model Evolution
    evolution = ModelEvolution(
        model_family="test-family",
        evolution_dir="./test_evolution",
    )
    print(f"‚úÖ ModelEvolution initialized")
    print(f"   - Strategy: {evolution.strategy.value}")
    
    # Auto Scaler
    scaler = AutoScalingController(
        mode=ScalingMode.BALANCED,
        min_batch_size=32,
        max_batch_size=512,
    )
    print(f"‚úÖ AutoScalingController initialized")
    print(f"   - Mode: {scaler.mode.value}")
    print(f"   - Batch size range: {scaler.min_batch_size}-{scaler.max_batch_size}")
    
    # Improvement Tracker
    tracker = ImprovementTracker(
        tracking_dir="./test_tracking",
    )
    print(f"‚úÖ ImprovementTracker initialized")
    print(f"   - Benchmarks: {len(tracker.benchmark_suite.benchmarks)}")
    
except Exception as e:
    print(f"‚ùå Training component error: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# Test 5: Test gradient computation and aggregation
print("\n[5/6] Testing gradient computation & aggregation...")
try:
    from actuallyopenai.training import WorkerUpdate, GradientPacket
    
    # Simulate gradient computation
    model.zero_grad()
    logits, loss = model(input_ids, labels=labels)
    loss.backward()
    
    # Collect gradients
    gradients = {
        name: param.grad.clone()
        for name, param in model.named_parameters()
        if param.grad is not None
    }
    
    print(f"‚úÖ Computed gradients for {len(gradients)} parameters")
    
    # Test gradient packet
    packet = GradientPacket(
        worker_id="test-worker-1",
        task_id="test-task-1",
        gradients=gradients,
        loss=loss.item(),
        batch_size=batch_size,
        compute_time=1.5,
    )
    print(f"‚úÖ GradientPacket created")
    print(f"   - Loss: {packet.loss:.4f}")
    print(f"   - Hash: {packet.gradient_hash}")
    
    # Test worker update
    update = WorkerUpdate(
        worker_id="test-worker-1",
        round_id=0,
        gradients=gradients,
        num_samples=batch_size,
        local_loss=loss.item(),
    )
    print(f"‚úÖ WorkerUpdate created")
    
except Exception as e:
    print(f"‚ùå Gradient test error: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# Test 6: Test async components
print("\n[6/6] Testing async operations...")

async def test_async():
    try:
        # Initialize trainer
        await trainer.initialize()
        print(f"‚úÖ Trainer initialized")
        print(f"   - Phase: {trainer.state.phase.value}")
        print(f"   - Global step: {trainer.state.global_step}")
        
        # Start a new generation
        gen = await evolution.start_new_generation(model)
        print(f"‚úÖ Started generation {gen.generation_number}")
        print(f"   - ID: {gen.id}")
        print(f"   - Parameters: {gen.parameter_count:,}")
        
        # Take a compute snapshot
        test_workers = {
            "worker-1": {"status": "online", "compute_score": 50, "has_gpu": True, "vram_gb": 8},
            "worker-2": {"status": "training", "compute_score": 30, "has_gpu": False, "vram_gb": 0},
        }
        snapshot = await scaler.take_snapshot(test_workers)
        print(f"‚úÖ Compute snapshot taken")
        print(f"   - Active workers: {snapshot.active_workers}")
        print(f"   - Total compute: {snapshot.total_compute_power}")
        print(f"   - Resource tier: {scaler.current_tier.value}")
        
        # Get scaling parameters
        params = scaler.get_current_parameters()
        print(f"‚úÖ Scaling parameters:")
        print(f"   - Batch size: {params['global_batch_size']}")
        print(f"   - Learning rate: {params['effective_learning_rate']}")
        print(f"   - Mixed precision: {params['mixed_precision']}")
        
        # Get trainer status
        status = trainer.get_status()
        print(f"‚úÖ Trainer status:")
        print(f"   - Running: {status['is_running']}")
        print(f"   - Phase: {status['phase']}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Async test error: {e}")
        import traceback
        traceback.print_exc()
        return False

# Run async tests
success = asyncio.run(test_async())

# Summary
print("\n" + "=" * 60)
if success:
    print("üéâ ALL TESTS PASSED!")
    print("=" * 60)
    print("\nThe self-improving AI training system is working correctly.")
    print("\nKey capabilities verified:")
    print("  ‚úÖ Model architecture (transformer with RoPE, RMSNorm, SwiGLU)")
    print("  ‚úÖ Forward pass and loss computation")
    print("  ‚úÖ Gradient computation for distributed training")
    print("  ‚úÖ Continuous training engine")
    print("  ‚úÖ Federated learning aggregator")
    print("  ‚úÖ Model evolution tracking")
    print("  ‚úÖ Auto-scaling controller")
    print("  ‚úÖ Improvement tracking system")
    print("\nThe AI will continuously improve as more compute is contributed! üöÄ")
else:
    print("‚ùå SOME TESTS FAILED")
    print("=" * 60)
    sys.exit(1)
